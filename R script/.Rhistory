<<<<<<< Updated upstream
textcat(),
NA)(URL))
df_subset_lang <- df_subset[90:110,] %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(URL),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
df_subset[90:110,]
df_subset<-select(df[90:110,], URL)
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(URL),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
View(df_subset_lang)
install.packages("pbapply")
library(pbapply)
pbsapply(df_subset, function(url){
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
=======
for(i in 1:len) {
page<- read_html(urlvector[i])
appinfo<-page%>%
rvest::html_nodes(".section__description .we-clamp__contents")%>%
rvest::html_text(trim=TRUE)%>%
textcat()
if(appinfo!="english") {
death_vector<-c(death_vector,i)
}
}
death_vector[1]
urlvector[-2]
tibble(url=urlvector)
tibble(url=urlvector)
language<-NULL
urlgiappo<-"https://itunes.apple.com/us/app/%D1%8F%D1%81%D0%BC%D0%BE%D0%B3%D1%83/id1435606589?mt=8"
urlenglish<- "https://itunes.apple.com/us/app/human-anatomy-atlas-2019/id1117998129?mt=8"
urlrussian<- "https://itunes.apple.com/us/app/cosme-%E5%8C%96%E7%B2%A7%E5%93%81-%E3%82%B3%E3%82%B9%E3%83%A1%E3%81%AE%E3%83%A9%E3%83%B3%E3%82%AD%E3%83%B3%E3%82%B0-%E3%82%AF%E3%83%81%E3%82%B3%E3%83%9F/id793054713?mt=8"
urlvector<-c(urlenglish,urlrussian,urlgiappo)
tibble(url=urlvector)
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
df<-tibble(url=urlvector)
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
df
df<-tibble(url=urlvector, language=language)
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
for(i in 1:len) {
page<- read_html(urlvector[i])
appinfo<-page%>%
rvest::html_nodes(".section__description .we-clamp__contents")%>%
rvest::html_text(trim=TRUE)%>%
textcat()
language<-c(language,appinfo)
}
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
df
df_english_only<-df[which(language=="english")]
View(df_english_only)
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
View(df_english_only)
read.csv2("app_M_H&F.csv")
df<-read.csv2("app_M_H&F.csv")
View(df)
df_url_subset<-subset(df_all_languages[,2]
df_url_subset<-subset(df_all_languages[,2])
df_url_subset<-df_all_languages[,2]
df_all_languages<-read.csv2("app_M_H&F.csv")
df_url_subset<-df_all_languages[,2]
df_url_subset<-df_all_languages[2,]
View(df_url_subset)
df_url_subset<-df_all_languages[,select(df_all_languages$URL)]
df_url_subset<-df_all_languages[,df_all_languages$URL]
df_url_subset<-df_all_languages$URL
language<-NULL
df_all_languages<-read.csv2("app_M_H&F.csv")
df_url_subset<-df_all_languages$URL
df_url_subset<-select(df_all_languages, df_all_languages$URL)
df_url_subset<-df_url_subset[,df_all_languages$URL]
df_url_subset<-df_url_subset[,"URL"]
library(dplyr)
df_url_subset<-select(df_all_languages, URL)
View(df_url_subset)
len<-length(df_url_subset$URL)
len
page<- read_html(df_url_subset$URL[2])
df_sub<-select(df_all_languages, URL)
library(rvest)
library(dplyr)
library(textcat)
language<-NULL
df_all_languages<-read.csv2("app_M_H&F.csv")
df_sub<-select(df_all_languages, URL)
df_sub[2]
df_sub[[2]]
df_sub[3,1]
View(df_all_languages)
df_all_languages<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
View(df_all_languages)
df_sub<-select(df_all_languages, URL)
df_sub[3,1]
df_sub[3]
df_sub[[3]]
df_subset<-select(df_all_languages, URL)
library(rvest)
library(textcat)
library(dplyr)
language<-NULL
df_subset<-select(df_all_languages, URL)
df_all_languages<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
len<-length(df_url_subset$URL)
library(rvest)
library(textcat)
library(dplyr)
language<-NULL
df_all_languages<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
df_subset<-select(df_all_languages, URL)
len<-length(df_url_subset$URL)
len<-length(df_subset$URL)
len
df_subset$URL[1]
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
df_subset<-select(df_all_languages, URL)
language<-NULL
df_subset<-select(df_all_languages, URL)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
len<-length(df_subset$URL)
df_subset$URL[1]
len
for(i in 1:len) {
page<- read_html(df_subset$URL[i])
appinfo<-page%>%
rvest::html_nodes(".section__description .we-clamp__contents")%>%
rvest::html_text(trim=TRUE)%>%
textcat()
language[i]<-appinfo
}
df_all_languages<-df%>%
mutate(language=language)
install.packages("tictoc")
library(tictoc)
tic()
language<-NULL
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
df_subset<-select(df_all_languages, URL)
df_subset$URL[1]
len<-length(df_subset$URL)
len
toc()
tic()
for(i in 1:len) {
print(i)
page<- read_html(df_subset$URL[i])
appinfo<-page%>%
rvest::html_nodes(".section__description .we-clamp__contents")%>%
rvest::html_text(trim=TRUE)%>%
textcat()
language[i]<-appinfo
}
toc()
last(language)
anyDuplicated(df)
source('~/GitHub/E-HEALTH/R script/mergetables.r', encoding = 'UTF-8', echo=TRUE)
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
#Esistono elementi doppi nel dataframe?
anyDuplicated.data.frame(my.df)
#Esistono elementi doppi nel dataframe?
anyDuplicated.data.frame(my.df)
#Sembra di no, per sicurezza si eliminano i duplicati
my.new.df <- my.df[!duplicated(paste(my.df$Name, my.df$URL, my.df$ID, my.df$X)),]
library(rvest)
library(textcat)
library(dplyr)
library(tictoc)
language<-NULL
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
View(my.df)
anyDuplicated(my.df)
#Sembra di no, per sicurezza si eliminano i duplicati
my.new.df <- my.df[!duplicated(paste(my.df$Name, my.df$URL, my.df$ID, my.df$X)),]
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE, row.names=FALSE)
page<- read_html(df_subset$URL[106])
page<-read_html(curl(df_subset$URL[106], handle = curl::new_handle("useragent" = "Mozilla/5.0")))
library(curl)
page<-read_html(curl(df_subset$URL[106], handle = curl::new_handle("useragent" = "Mozilla/5.0")))
source('~/GitHub/E-HEALTH/R script/language_detector.r', echo=TRUE)
try(
page<- read_html(df_subset$URL[106])
appinfo<-page%>%
rvest::html_nodes(".section__description .we-clamp__contents")%>%
rvest::html_text(trim=TRUE)%>%
textcat()
language[i]<-appinfo
)
try(
page<- read_html(df_subset$URL[106]))
tic()
for(i in 1:len) {
print(i)
try(
page<- read_html(df_subset$URL[i]))
appinfo<-page%>%
rvest::html_nodes(".section__description .we-clamp__contents")%>%
rvest::html_text(trim=TRUE)%>%
textcat()
language[i]<-appinfo
}
toc()
library(rvest)
library(textcat)
library(dplyr)
library(tictoc)
library(curl)
language<-NULL
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE, row.names=FALSE)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df_all_languages, URL)
>>>>>>> Stashed changes
library(rvest)
library(textcat)
library(dplyr)
library(tictoc)
library(curl)
library(tidyverse)
library(pbapply)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
library(rvest)
library(textcat)
library(dplyr)
library(tictoc)
library(curl)
library(tidyverse)
library(pbapply)
language<-NULL
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df[90:110,], URL)
df_subset$URL
len<-length(df_subset$URL)
len
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
df_all_languages<-df%>%
mutate(language=language)
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
2+2
library(rvest)
library(textcat)
library(dplyr)
library(tictoc)
library(curl)
library(tidyverse)
library(pbapply)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df[90:110,], URL)
df_subset$URL
len<-length(df_subset$URL)
len
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
View(df_subset)
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(df_subset$URL))
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(A))
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(url),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(URL),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(URL = as.character(URL),    # Convert factors to character for read_html
languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
View(df_subset)
pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
df_subset_lang <-pbsapply(df_subset, function(url){
df_subset_lang <- df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
df_subset_lang <-pbsapply(df_subset, function(url){
df_subset %>% rowwise() %>%     # Evaluate each row (URL) separately
mutate(languages = possibly(~.x %>% read_html() %>%    # Try to take a URL, read it,
html_nodes(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)%>%
textcat(),
NA)(URL))
})
View(df_subset_lang)
progress_estimated(
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
)
View(df_subset_lang)
require("progress")
install.packages("progress")
library(progress)
language<-NULL
pb <- progress_bar$new(
format = "  downloading [:bar] :percent in :elapsed",
total = 100, clear = FALSE, width= 60)
for (i in 1:100) {
pb$tick()
Sys.sleep(1 / 100)
}
p <- progress_estimated(3)
p$tick()
p$tick()
p$tick()
p$tick()
p<-progress_estimated(123347)
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p$tick()
p<-progress_estimated(123347)
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat()%>%
p$tick,
NA)(URL))
p<-progress_estimated(123347)
function() {
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
p$tick
}
bigdata<-function() {
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
p$tick
}
bigdata()
pb <- progress_bar$new(
format = "(:spin) [:bar] :percent",
total = 30, clear = FALSE, width = 60)
for (i in 1:30) {
pb$tick()
Sys.sleep(3 / 100)
}
pb <- progress_bar$new(
format = "  downloading :what [:bar] :percent eta: :eta",
clear = FALSE, total = 200, width = 60)
f <- function() {
for (i in 1:100) {
pb$tick(tokens = list(what = "foo   "))
Sys.sleep(2 / 100)
}
for (i in 1:100) {
pb$tick(tokens = list(what = "foobar"))
Sys.sleep(2 / 100)
}
}
f()
Sys.sleep(2)
library(tidyverse)
arduously_long_nchar <- function(input_var, .pb=NULL) {
if ((!is.null(.pb)) && inherits(.pb, "Progress") && (.pb$i < .pb$n)) .pb$tick()$print()
Sys.sleep(1)
nchar(input_var)
}
pb <- progress_estimated(length(letters))
map_int(letters, arduously_long_nchar, .pb=pb)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df[90:110,], URL)
df_subset$URL
library(rvest)
library(textcat)
library(dplyr)
library(tictoc)
library(curl)
library(tidyverse)
library(progress)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df[90:110,], URL)
df_subset$URL
len<-length(df_subset$URL)
len
df_subset<-select(df, URL)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df, URL)
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
<<<<<<< Updated upstream
warnings()
df_subset<-sample(df_subset, 2000)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df, URL)
df_subset<-sample(df_subset, 2000)
df_subset<-sample(df_subset, 2000, replace=TRUE)
df_subset<-sample(df_subset, 2000, replace=TRUE)
df_subset<-sample(df_subset, 20, replace=TRUE)
df_subset_random<-sample(df_subset, 20, replace=TRUE)
df_subset_random<-sample(df_subset, 1500)
df_subset_random<-df_subset[sample(nrow(df_subset), "1500"), ]
View(df_subset_random)
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
df_subset<-select(df, URL)
df_subset_random<-df_subset[sample(nrow(df_subset), "1500"), ]
df_subset<-select(df, URL)
df_subset[sample(nrow(df_subset), "1500"), ]
df_subset<-df_subset[sample(nrow(df_subset), "1500"), ]
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
toc()
tibble(URL=df_subset)
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
df_subset<-select(df, URL)
df_subset<-df_subset[sample(nrow(df_subset), "1500"), ]
df_subset<-tibble(URL=df_subset)
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
df_subset<-df_subset[sample(nrow(df_subset), "100"), ]
df_subset<-select(df, URL)
df_subset<-df_subset[sample(nrow(df_subset), "100"), ]
df_subset<-tibble(URL=df_subset)
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
####RUN####
df_subset<-df_subset[sample(nrow(df_subset), "50"), ]
####RUN####
df_subset<-df_subset[sample(nrow(df_subset), "50"), ]
####RUN####
df_subset<-df_subset[sample(nrow(df_subset), "50"), ]
df_subset<-df_subset[sample(nrow(df_subset), "50"), ]
df_subset<-tibble(URL=df_subset)
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
toc()
View(df_subset_lang)
####RUN####
df_subset<-df_subset[sample(nrow(df_subset), "3000"), ]
df<-read.csv2("app_M_H&F.csv", stringsAsFactors=FALSE)
anyDuplicated(df)
####RUN####
df_subset<-select(df, URL)
####RUN####
df_subset<-select(df, URL)
df_subset<-df_subset[sample(nrow(df_subset), "3000"), ]
df_subset<-tibble(URL=df_subset)
tic()
df_subset_lang <- df_subset %>%
rowwise() %>%
mutate(languages = possibly(~.x %>%
read_html() %>%
html_nodes(".section__description .we-clamp__contents") %>%
html_text(trim=TRUE) %>%
textcat(),
NA)(URL))
toc()
####r####
df_english_only<-df_subset_lang[which(language=="english"),]
View(df_subset_lang)
####r####
tic()
toc()
tic()
df_english_only<-df_subset_lang[which(languages=="english"),]
toc()
##
tic()
attach(df_subset_lang)
df_english_only<-df_subset_lang[which(languages=="english"),]
detach()
toc()
write.csv2(my.new.df, "app_M_H&F_ENGLISH_ONLY.csv",row.names=FALSE)
View(df_subset_lang)
View(df)
View(df_english_only)
write.csv2(df_english_only, "app_M_H&F_ENGLISH_ONLY.csv",row.names=FALSE)
write.csv2(df_subset_lang, "app_M_H&F_ALL_LANGUAGES.csv",row.names=FALSE)
anyDuplicated(df_english_only)
anyDuplicated(df_subset_lang)
if(anyDuplicated(df_subset_lang)>0){
df_subset_lang <- df_subset_lang[!duplicated(paste(df_subset_lang$URL, df_subset_lang$languages)),]
}
anyDuplicated(df_subset_lang)
attach(df_subset_lang)
df_english_only<-df_subset_lang[which(languages=="english"),]
detach()
anyDuplicated(df_english_only)
write.csv2(df_subset_lang, "app_M_H&F_ALL_LANGUAGES.csv",row.names=FALSE)
write.csv2(df_english_only, "app_M_H&F_ENGLISH_ONLY.csv",row.names=FALSE)
df_app<-select(df$Name, df$ID, df_english_only)
View(df)
df_app<-inner_join(df, df_english_only, by(URL=URL))
df_app<-inner_join(df, df_english_only, by("URL"="URL"))
df_app<-inner_join(df, df_english_only, by="URL"="URL")
df_app<-inner_join(df, df_english_only, by=c("URL"="URL")
df_app<-inner_join(df, df_english_only, by=c("URL"="URL"))
df_app<-inner_join(df, df_english_only, by="URL")
View(df_app)
anyDuplicated(df_app)
if(anyDuplicated(df_app)>0){
df_app <- df_app[!duplicated(paste(df_app$Name,df_app$URL, df_app$ID, df_app$languages)),]
}
anyDuplicated(df_app)
if(anyDuplicated(df_subset_lang)>0){
df_subset_lang <- df_subset_lang[!duplicated(paste(df_subset_lang$URL, df_subset_lang$languages)),]
}
attach(df_subset_lang)
df_english_only<-df_subset_lang[which(languages=="english"),]
detach()
toc()
tic()
df_app<-inner_join(df, df_english_only, by="URL")
if(anyDuplicated(df_app)>0){
df_app <- df_app[!duplicated(paste(df_app$Name,df_app$URL, df_app$ID, df_app$languages)),]
}
toc()
tic()
df_app<-inner_join(df, df_english_only, by="URL")
toc()
df_app<-inner_join(df, df_english_only, by="URL")
toc()
tic()
df_app<-inner_join(df, df_english_only, by="URL")
toc()
tic()
if(anyDuplicated(df_app)>0){
df_app <- df_app[!duplicated(paste(df_app$Name,df_app$URL, df_app$ID, df_app$languages)),]
}
toc()
write.csv2(df_subset_lang, "subset_M_H&F_ALL_LANGUAGES.csv",row.names=FALSE)
write.csv2(df_english_only, "subset_M_H&F_ENGLISH_ONLY.csv",row.names=FALSE)
write.csv2(df_app, "app_M_H&F_ENGLISH_ONLY.csv",row.names=FALSE)
=======
2+2
url<-"https://itunes.apple.com/us/app/human-anatomy-atlas-2019/id1117998129?mt=8"
page<-read_html(url)
source('~/Documents/GITHUB/E-HEALTH/R script/getappinfo.r', echo=TRUE)
#punteggio medio (DOUBLE)####
avgrating<-page%>%
html_nodes(".we-customer-ratings__averages__display")%>%
html_text(trim = TRUE)%>%
as.numeric(.)
#punteggio totale (DOUBLE)####
ratings<-page%>%
html_node(".we-customer-ratings__count")%>%
html_text(trim = TRUE)%>%
gsub(" Ratings", "", .)
ratings<-as.numeric(sub("K", "e3", ratings))
pegipattern<-"\\d+"
pegi<-page%>%
html_node(".l-row:nth-child(6) .large-6")%>%
html_text(trim=TRUE)%>%
str_extract(., pattern = pegipattern)%>%
as.numeric()
category<-page%>%
html_node(".large-6 .link")%>%
html_text(trim=TRUE)
description<-page%>%
html_node(".section__description .we-clamp__contents")%>%
html_text(trim=TRUE)
>>>>>>> Stashed changes
