#sep = \n",
eol = "\n\n"
)
rm(list = ls())
library(rvest)
descr_train<- read_html ("Across_Specialties3.txt")
descr_freq_words<-findFreqTerms(descr_train, 5)
descr_freq_words<-findFreqTerms(descr_train)
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
install.packages("tm")
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
descr_train<- read_html ("Across_Specialties3.txt")
descr_freq_words<-findFreqTerms(descr_train)
source('~/GitHub/E-HEALTH/prova_freq.R')
descr_freq_words<-findFreqTerms(descr_train, 5)
source('~/GitHub/E-HEALTH/prova_freq.R')
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(1,4,5)]
source('~/GitHub/E-HEALTH/prova_freq.R')
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
t4_train$NC.1.0<-factor(t4_train$NC.1.0)
t4_train
descr_freq_words<-findFreqTerms(t4_train, 5)
source('~/GitHub/E-HEALTH/prova_freq.R')
descr_freq_words<-findFreqTerms(t4_train)
descr_corpus <- VCorpus(VectorSource(t4_train$Description))
descr_corpus <- VCorpus(VectorSource(t4_train$description))
t4_train
descr_corpus <- VCorpus(VectorSource(t4_train)
source('~/GitHub/E-HEALTH/prova_freq.R')
descr_corpus <- VCorpus(VectorSource(t4_train)
s
t4_train[0]
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
descr_freq_words<-findFreqTerms(t4_train)
help("findFreqTerms")
descr_freq_words<-findFreqTerms(t4_train,lowfreq = 0, highfreq = Inf)
N<-matrix(t4_train,1);
View(N)
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
N<-matrix(t4_train,inf);
N<-matrix(t4_train, Inf);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
source('~/GitHub/E-HEALTH/prova_freq.R')
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(t4_train,lowfreq = 0, highfreq = Inf)
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
source('~/GitHub/E-HEALTH/prova_freq.R')
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
View(N)
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
# make corpus for text mining (data comes from package, for reproducibility)
corpus <- Corpus(VectorSource(training_test_4))
View(corpus)
source('~/GitHub/E-HEALTH/prova_freq.R')
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1)
findFreqTerms(a.dtm1,2)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
m <- as.matrix(training_test_4)
v <- sort(rowSums(m), decreasing=TRUE)
source('~/GitHub/E-HEALTH/prova_freq.R')
df <-
read.csv2(
"Across_Specialties_nostro.csv",
stringsAsFactors = F,
header = T
)
df <- df[,4]
write.table(
df,
"Across_Specialties_nostro.txt",
append = F,
dec = ".",
col.names = F,
row.names = F,
#sep = \n",
eol = "\n\n"
)
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
install.packages("reshape2")
reshape2)
library(reshape2)
install.packages("reshape2")
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
metaout <- read_html("Across_nostro_out.xml")
candidate_preferred <- metaout %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
frequenza <- read_html("Across_Specialties3.txt")
df1 <- count(df, frequenza)
View(frequenza)
View(frequenza)
#leggo l'output di metamap
frequenza <- read_html("Across_Specialties3.cvs")
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
frequenza <- read_html("Across_Specialties3.cvs")
#leggo l'output di metamap
frequenza <- read_cvs("Across_Specialties3.cvs")
#leggo l'output di metamap
frequenza <- read.csv2("Across_Specialties3.cvs")
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
frequenza <- read.csv2("Across_Specialties3.cvs")
#leggo l'output di metamap
frequenza <- read.csv2("Across_Specialties3.cvs")
#leggo l'output di metamap
frequenza <- read_html("Across_Specialties3.txt")
candidate_preferred <- metaout %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
candidate_preferred <- frequenza %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
df <- tibble(
"Candidate_Preferred" = candidate_preferred
)
df1 <- count(df, df)
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
metaout <- read_html("Across_Specialties3.txt")
candidate_preferred <- metaout %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
df <- tibble(
"Candidate_Preferred" = candidate_preferred
)
df1 <- count(df, "Candidate_Preferred")
df1_sub<-df1 %>% filter(freq >= 10)
View(df1)
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
# make corpus for text mining (data comes from package, for reproducibility)
corpus <- Corpus(VectorSource(training_test_4))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1,2)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
metaout <- read_html("Across_Specialties3.txt")
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
invalue <- read_html("Across_Specialties3.txt")
prova <- invalue %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
df <- tibble(
"cate" = prova
)
df1 <- count(df, "cate")
df1_sub<-df1 %>% filter(freq >= 10)
View(df1_sub)
#SECONDO METODO
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
corpus <- Corpus(VectorSource(training_test_4))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1,2)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
# make corpus for text mining (data comes from package, for reproducibility)
corpus <- Corpus(VectorSource(training_test_4))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1,2)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
#PRIMO METODO
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
invalue <- read_html("Across_Specialties3.txt")
prova <- invalue %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
df <- tibble(
"cate" = prova
)
df1 <- count(df, "cate")
df1_sub<-df1 %>% filter(freq >= 10)
View(df1_sub)
#PRIMO METODO
rm(list = ls())
library(rvest)
library(tidyverse)
library(pbapply)
library(reshape2)
library(ggplot2)
library(data.table)
#leggo l'output di metamap
invalue <- read_html("Across_Specialties3.txt")
View(invalue)
prova <- invalue %>%
html_nodes("candidatepreferred") %>%
html_text(trim = F)
df <- tibble(
"cate" = prova
)
df1 <- count(df, "cate")
df1_sub<-df1 %>% filter(freq >= 10)
#SECONDO METODO
rm(list = ls())
library(rvest)
library(tidyverse)
library(tm)
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
corpus <- Corpus(VectorSource(training_test_4))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1,2)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
#SECONDO METODO
rm(list = ls())
training_test_4<-read.csv2("Across_Specialties3.csv",header = T)
View(training_test_4)
corpus <- Corpus(VectorSource(training_test_4))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
View(a)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1,2)
training_test_4<-read.csv2("Across_Specialties.csv",header = T)
corpus <- Corpus(VectorSource(training_test_4))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
findFreqTerms(a.dtm1,2)
m <- as.matrix(a.dtm1)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
N<-matrix(t4_train);
descr_freq_words<-findFreqTerms(N,lowfreq = 0, highfreq = Inf)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
View(a.dtm1)
#SECONDO METODO
rm(list = ls())
training_test_4<-read.csv2("Across_Specialties.csv",header = T)
corpus <- Corpus(VectorSource(training_test_4))
View(training_test_4)
View(corpus)
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
a <- tm_map(corpus, FUN = tm_reduce, tmFuns = funcs)
View(a)
View(a)
View(a)
a.dtm1 <- TermDocumentMatrix(a, control = list(wordLengths = c(3,10)))
View(a.dtm1)
View(a.dtm1)
a.dtm1 <- TermDocumentMatrix(a)
findFreqTerms(a.dtm1,2)
a.dtm1 <- TermDocumentMatrix(a)
findFreqTerms(a.dtm1,2)
#SECONDO METODO
rm(list = ls())
training_test_4<-read.csv2("Across_Specialties.csv",header = T)
corpus <- Corpus(VectorSource(training_test_4))
stopwords <- c(stopwords("en"), "app", "can", "use","will","may")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords)
#corpus <- tm_map(corpus, removeNumbers)
}
corpus_clean<-clean_corpus(corpus)
a.dtm1 <- DocumentTermMatrix(a)
corpus_dtm <- DocumentTermMatrix(corpus_clean)
findFreqTerms(corpus_dtm,lowfreq = 10)
findFreqTerms(corpus_dtm,lowfreq = 2)
3
findFreqTerms(corpus_dtm,lowfreq = 3)
freq_termsfindFreqTerms(corpus_dtm,lowfreq = 3)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 3)
corpus_dtm <- TermDocumentMatrix(corpus_clean)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 3)
corpus_clean<-clean_corpus(corpus)
corpus_dtm <- TermDocumentMatrix(corpus_clean)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 3)
m <- as.matrix(corpus_dtm)
View(m)
#SECONDO METODO
rm(list = ls())
training_test_4<-read.csv2("Across_Specialties.csv",header = T)
View(training_test_4)
training_test_4<-read.csv2("Across_Specialties.csv",header = T)%>%
.[,4]
training_test_4<-read.csv2("Across_Specialties.csv",header = T)%>%
.[,4]%>%
as.character(.)
#SECONDO METODO
rm(list = ls())
training_test_4<-read.csv2("Across_Specialties.csv",header = T)
training_test_4<-read.csv2("Across_Specialties.csv",header = T)%>%
drop_na(.)
training_test_4<-read.csv2("Across_Specialties.csv",header = T)%>%
drop_na()
training_test_4<-read.csv2("Across_Specialties.csv",header = T)
training_test_4$Description <-
as.character(training_test_4$Description)
corpus <- Corpus(VectorSource(training_test_4$Description))
stopwords <- c(stopwords("en"), "app", "can", "use","will","may")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords)
#corpus <- tm_map(corpus, removeNumbers)
}
corpus_clean<-clean_corpus(corpus)
corpus_clean<-clean_corpus(corpus)
corpus_dtm <- DocumentTermMatrix(corpus_clean)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 3)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 50)
m <- as.matrix(corpus_dtm)
m <- as.matrix(corpus_dtm)
v <- sort(rowSums(m), decreasing=TRUE)
freq_terms
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 100)
freq_terms
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 1000)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 500)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 600)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 200)
corpus_tdm<-TermDocumentMatrix(corpus_clean)
m <- as.matrix(corpus_tdm)
corpus_tdm<-TermDocumentMatrix(corpus_clean)
m <- as.matrix(corpus_tdm)
library(SnowballC)
library(tm)
corpus_tdm<-TermDocumentMatrix(corpus_clean)
m <- as.matrix(corpus_tdm)
View(training_test_4)
corpus <- Corpus(VectorSource(training_test_4$Description[1,429]))
corpus <- Corpus(VectorSource(training_test_4$Description[1:429]))
stopwords <- c(stopwords("en"), "app", "can", "use","will","may")
clean_corpus <- function(corpus) {
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removeWords, stopwords)
#corpus <- tm_map(corpus, removeNumbers)
}
corpus_clean<-clean_corpus(corpus)
corpus_dtm <- DocumentTermMatrix(corpus_clean)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 200)
corpus_tdm<-TermDocumentMatrix(corpus_clean)
m <- as.matrix(corpus_tdm)
v <- sort(rowSums(m), decreasing=TRUE)
head(v, N)
head(v)
t4_train<-training_test_4[which(training_test_4$X=="Test"),c(4)]
View(corpus_dtm)
View(corpus_tdm)
View(corpus_tdm)
View(corpus_tdm)
m <- as.matrix(corpus_dtm)
v <- sort(rowSums(m), decreasing=TRUE)
m <- as.matrix(corpus_tdm)
v <- sort(rowSums(m), decreasing=TRUE)
head(v)
freq_terms<-findFreqTerms(corpus_dtm,lowfreq = 100)
corpus_tdm<-TermDocumentMatrix(corpus_clean)
m <- as.matrix(corpus_tdm)
v <- sort(rowSums(m), decreasing=TRUE)
head(v)
v
v <- sort(rowSums(m), decreasing=F)
head(v)
v <- sort(rowSums(m), decreasing=T)
head(v)
v
View(m)
barplot(v[1:30], col = "tomato", las = 2,horiz = F)
barplot(v[1:30], col = "tomato", las = 2,horiz = T)
head(v)
head(v[1:50])
head(v[1:50,])
head(v)[1:50]
head(v[1:50])
v[1:50]
